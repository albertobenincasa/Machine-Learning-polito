{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAXIMUM LIKELIHOOD ESTIMATION\n",
    "\n",
    "I have a coin. If I flip it, what's the probability it will fall with the head up?\n",
    "\n",
    "Data D={H, T, H, H, T}\n",
    "\n",
    "Flips are independent events and identically distribuited according to Bernoulli distribution.\n",
    "Goal of MLE $\\to$ choose $\\theta$ that maximizes the probability of the observed data.\n",
    "\n",
    "$$ \\hat{\\theta} = \\underset{\\theta}{argmax}{P(D|\\theta)} = \\underset{\\theta}{argmax}{\\prod_{i=1}^{n} P(x_i|\\theta)} = $$\n",
    "\n",
    "$$ = \\underset{\\theta}{argmax}{\\prod_{i:x_i=H} \\theta \\prod_{i:x_i=T} (1-\\theta)} = \\underset{\\theta}{argmax}{\\theta^{\\alpha_H}(1-\\theta)^{\\alpha_T}} \\to J(\\theta) $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta} = \\alpha_H\\theta^{\\alpha_H-1}(1-\\theta) - \\alpha_T\\theta^{\\alpha_H}(1-\\theta)^{\\alpha_T-1} = 0 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_H(1-\\theta) - \\alpha_T\\theta = 0 \\to \\hat\\theta_{MLE} = \\frac{\\alpha_H}{\\alpha_H+\\alpha_T}\n",
    "$$\n",
    "\n",
    "We know that coin is \"close\" to 50-50! Rather then estimating a single $\\theta$ we obtain a distribution over possible value of $\\theta$ \n",
    "\n",
    "## MAXIMUM A POSTERIORI ESTIMATION\n",
    "\n",
    "In the coin flip problem, the likelihood is a Binomial distribution: $\\begin{pmatrix} n \\\\ \\alpha_H \\end{pmatrix}\\theta^{\\alpha_H}(1-\\theta)^{\\alpha_T}$\n",
    "\n",
    "If the prior is Beta distribution:\n",
    "\n",
    "$$\n",
    "P(\\theta) = \\frac{\\theta^{\\beta_H-1}(1-\\theta)^{\\beta_T-1}}{B(\\beta_H, \\beta_T)} \\sim Beta(\\beta_H, \\beta_T)\n",
    "$$\n",
    "\n",
    "Then, the posterior is Beta distribution: $P(\\theta|D) \\sim Beta(\\beta_H, \\beta_T)$\n",
    "\n",
    "$P(\\theta)$ and $P(\\theta|D)$ have the same form! \n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{MAP} = \\underset{\\theta}{argmax}{P(\\theta|D)} = \\underset{\\theta}{argmax}{P(D|\\theta)P(\\theta)} = \\frac{\\alpha_H+\\beta_H-1}{\\alpha_H+\\beta_H+\\alpha_T+\\beta_T-2} \n",
    "$$\n",
    "\n",
    "## MLE VS MAP\n",
    "\n",
    "* Maximum Likelihood Estimation (MLE) $\\to$ Choose value that maximizes the probability of observed data. $$ \\hat{\\theta}_{MLE} = \\underset{\\theta}{argmax}P(D|\\theta) $$ \n",
    "\n",
    "* Maximum *a posteriori* estimation (MAP) $\\to$ Choose value that is most probable given observed data and prior belief $$ \\hat{\\theta}_{MAP} = \\underset{\\theta}{argmax}P(\\theta|D) = \\underset{\\theta}{argmax}P(D|\\theta)P(\\theta) $$\n",
    "\n",
    "### MLE for Gaussian mean and variance\n",
    "\n",
    "The Gaussian distribution is defined by the following:\n",
    "\n",
    "$$ P(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "We want to choose $\\theta = (\\mu, \\sigma^2)$ that maximizes the probability of observed data.\n",
    "\n",
    "$$ \\hat{\\theta}_{MLE} =  \\underset{\\theta}{argmax}P(D|\\theta) =  \\underset{\\theta}{argmax}\\prod_{i=1}^{n}P(x_i|\\theta) $$\n",
    "$$ =  \\underset{\\theta}{argmax}\\prod_{i=1}^{n}\\frac{1}{2\\sigma^2}e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}} $$\n",
    "$$ = \\underset{\\theta=(\\mu,\\sigma^2)}{argmax}\\frac{1}{2\\sigma^2}e^{-\\sum_{i=1}^{n}\\frac{(x_i-\\mu)^2}{2\\sigma^2}} \\to J(\\theta) $$\n",
    "\n",
    "So the parameters that maximizes the probability are:\n",
    "\n",
    "$$ \\hat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}x_i $$\n",
    "$$ \\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\hat\\mu)^2 $$\n",
    "\n",
    "## THE LAW OF LARGE NUMBERS\n",
    "\n",
    "This is a key fundamental result, which is so intuitive that is at the heart of the frequentists interpretation of probability, and it is essential to the field of statistic.\n",
    "Let $X_1,X_2,\\dots,X_n$ be an infinite sequence of i.i.d random variables, with mean $\\mu_x = E[X_i]$ \n",
    "\n",
    "Define the average $ \\overline{x}_n = \\frac{x_1+x_2+\\dots+x_n}{n} = \\frac{1}{x}\\sum_{i=1}^{n}x_i $\n",
    "\n",
    "Then, as $ n \\to \\infty $, the average converges to a non-random real number.\n",
    "\n",
    "$$ \\lim_{n \\to \\infty} \\overline{x}_n = \\mu_x $$\n",
    "\n",
    "## TCHEBICHEV LEMMA\n",
    "\n",
    "$$ \\mu = \\langle x \\rangle = \\int xP(x)dx $$\n",
    "\n",
    "$$ \\sigma^2 = \\langle x^2 \\rangle - \\mu^2 = \\int{(x-\\mu)^2P(x)dx} = \\int{x^2P(x)dx} - \\mu^2  $$\n",
    "\n",
    "$$ \\sigma^2 = \\int{(x-\\mu)^2P(x)dx} > \\int_{|x-\\mu|>t}{(x-\\mu)^2P(x)dx} > t^2 \\int_{|x-\\mu|>t}P(x)dx $$\n",
    "\n",
    "And so,\n",
    "\n",
    "$$ \\sigma^2 > t^2 \\int_{|x-\\mu|>t}P(x)dx \\to \\int_{|x-\\mu|>t}P(x)dx < \\frac{\\sigma^2}{t^2} $$\n",
    "\n",
    "## DEMONSTRATION OF THE LAW OF LARGE NUMBERS\n",
    "\n",
    "We know that $ V(A+B) = V(A) + V(B) $ if $A,B$ are independent.\n",
    "\n",
    "Generalizing this for $N$ variables independent we obtain that the variance of the quantity $y_nN$ is $N\\sigma^2$ while its mean is obviously $N\\mu$. The Tchebicev lemma implies:\n",
    "\n",
    "$$ P(N|y_n - \\mu| > t) < \\frac{\\sigma^2}{t^2}N $$\n",
    "\n",
    "And, if we choose $t=N\\epsilon$ we have:\n",
    "\n",
    "$$ P(|y_n-\\mu| > \\epsilon) < \\frac{\\sigma^2}{N\\epsilon^2} $$\n",
    "\n",
    "## THE CENTRAL LIMIT THEOREM\n",
    "\n",
    "When discussing the normal distribution, we informally stated that the sum of independent random vriables resembles a normal random variable. Let $X_1,X_2,\\dots,X_n$ be a sequence of independent and identically distribuited random variables, with mean $ \\mu_x = E[X_i]$ and variance $\\sigma_x^2 = V(X_i)$. Define:\n",
    "\n",
    "$$ S_n = X_1+X_2+\\dots+X_n = \\sum_{i=1}^{n} X_i $$\n",
    "\n",
    "Note that $E[S]=n\\mu_x$ and $V(S)=n\\sigma_x^2$. Then, we have that the distribution of $S$ is approximately normal, that is:\n",
    "\n",
    "$$ S \\approx N(n\\mu_x, n\\sigma^2)$$\n",
    "\n",
    "## BAYES DECISION RULE\n",
    "\n",
    "$X$ is an observation for which:\n",
    "\n",
    "if $ P(\\omega_1|X) > P(\\omega_2|X) \\to$ true state of nature = $\\omega_1$\n",
    "\n",
    "if $ P(\\omega_1|X) < P(\\omega_2|X) \\to$ true state of nature = $\\omega_2$\n",
    "\n",
    "Whenever we observe a particular $X$, the probability of error is:\n",
    "\n",
    "$P(error|X) = P(\\omega_1|X)$ if we decide $\\omega_1$\n",
    "\n",
    "$P(error|X) = P(\\omega_2|X)$ if we decide $\\omega_2$\n",
    "\n",
    "Bayes Decision Rule minimizes probability of error: \n",
    "\n",
    "$P(error|X) = min[P(\\omega_1|X), P(\\omega_2|X)]$\n",
    "\n",
    "Generalization of the preceding ideas: we want to make use of more than one feature, use more than two states of nature, allowing actions and not only decide on the state of nature. We want also introduce a Loss function that is more general than the probability of error.\n",
    "\n",
    "Allowing actions other than classifications primarily allows the possibility of rejection. The Loss function states how costly each action taken is.\n",
    "\n",
    "Let ${\\omega_1, \\omega_2, \\dots, \\omega_c}$ be the set of $c$ states of nature.\n",
    "\n",
    "Let ${\\alpha_1, \\alpha_2, \\dots, \\alpha_A}$ be the set of $A$ possible actions.\n",
    "\n",
    "Let $\\lambda(\\alpha_i|\\omega_j)$ be the loss incurred for taking action $\\alpha_i$ when the true state of nature is $\\omega_j$. \n",
    "\n",
    "The overall risk is defined as:\n",
    "\n",
    "$$ R = \\sum_{i=1}^{A} R(\\alpha_i|X) $$\n",
    "\n",
    "And we want to minimize it.\n",
    "\n",
    "$$ R(\\alpha_i|X) = \\sum_{j=1}^{j=c}\\lambda(\\alpha_i|\\omega_j)P(\\omega_j|X)$$  \n",
    "This is the expected loss with action $i$. When $R$ is minimimum, we call it the Risk. The Bayes Risk is the best value that can be achieved.\n",
    "\n",
    "### TWO CATEGORY CLASSIFICATION\n",
    "\n",
    "$\\alpha_1=$ deciding $\\omega_1$.\n",
    "\n",
    "$\\alpha_2=$ deciding $\\omega_2$.\n",
    "\n",
    "$\\lambda_{ij} = \\lambda(\\alpha_i|\\omega_j) \\to$ the loss incurred for deciding $\\omega_i$ when the true state is $\\omega_j$.\n",
    "\n",
    "$$ R(\\alpha_1|X) = \\lambda_{11}P(\\omega_1|X) + \\lambda_{12}P(\\omega_2|X) $$\n",
    "$$ R(\\alpha_1|X) = \\lambda_{21}P(\\omega_1|X) + \\lambda_{22}P(\\omega_2|X) $$\n",
    "\n",
    "Our rule is the following: if $R(\\alpha_1|X) < R(\\alpha_2|X)$ then action $\\alpha_1$ is taken. We can rewrite this rule in the form:\n",
    "\n",
    "decide $\\omega_1$ if $ (\\lambda_{21} - \\lambda_{11})P(X|\\omega_1)P(\\omega_1) > (\\lambda_{12} - \\lambda_{22})P(X|\\omega_2)P(\\omega_2) $ and decide $\\omega_2$ otherwise. \n",
    "\n",
    "Actions are decision on classes: if action $\\alpha_i$ is taken and the true state of nature is $\\omega_j$, then the decision is correct if $i=j$ and in error if $i\\neq j$. Seek a decision rule that minimizes the probability of error, which is the error rate.\n",
    "\n",
    "## PRINCIPAL COMPONENT ANALYSIS\n",
    "\n",
    "The Principal Component Analysis has the goal to reduce the dimensionality of the data. We use the orthogonal projection of tha data onto a lower-dimension linear space that maximizes variance of projected data and minimizes mean squared distance between data points and projections. PCA vectors originate from the center of mass. \n",
    "\n",
    "Principal Component #1 $\\to$ points in the direction of the largest variance.\n",
    "\n",
    "Each subsequent PC $\\to$ is orthogonal to the previous ones and points in the direction of the largest variance of the residual subspace.\n",
    "\n",
    "### PCA ALGORITHM 1: SEQUENTIAL\n",
    "\n",
    "Given the centered (where centered means that all vectors start from the origin of the coordinate system) data ${x_1,\\dots,x_m}$, compute the principal vectors.\n",
    "\n",
    "First PCA vector:\n",
    "$$ w_1 = \\underset{\\|w\\|=1}{argmax}\\frac{1}{m}\\sum_{i=1}^{m}{(w^Tx_i)^2} $$\n",
    "\n",
    "We maximize the variance of projection of $x$. \n",
    "Second PCA vector:\n",
    "$$ w_2 = \\underset{\\|w\\|=1}{argmax}\\frac{1}{m}\\sum_{i=1}^{m}{[w^T(x_i - w_1w_1^Tx_i)]^2} $$\n",
    "\n",
    "Given $w_1,\\dots,w_{k-1}$ we calculate $w_k$ as before:\n",
    "$$ w_k = \\underset{\\|w\\|=1}{argmax}\\frac{1}{m}\\sum_{i=1}^{m}{[w^T(x_i - \\sum_{j=1}^{k-1}w_jw_j^Tx_i)]^2} $$\n",
    "\n",
    "### PCA ALGORITHM 2: SAMPLE COVARIANCE MATRIX\n",
    "\n",
    "Given data ${x_1,\\dots,x_m}$ compute the covariance matrix $\\Sigma$.\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{m}\\sum_{i=1}^{m}(x_i - \\overline{x})(x_i - \\overline{x})^T \\longrightarrow \\overline{x} = \\frac{1}{m}\\sum_{i=1}^{m}x_i $$\n",
    "\n",
    "The PCA basis vectors are the eigenvectors of $\\Sigma$. The larger eigenvalue the more important eigenvector. Most of the time, we have so many data and compute $\\Sigma$ is really expensive. We can use a clever workaround called eigenfaces trick!\n",
    "\n",
    "Use $L=X^TX$ instead of $\\Sigma = XX^T$. If $v$ is eigenvector of $L$, then $Xv$ is eigenvector of $\\Sigma$:\n",
    "$$ Lv = \\gamma v $$\n",
    "$$ X^TXv = \\gamma v $$\n",
    "$$ X(X^TXv) = \\gamma X v $$\n",
    "$$ (XX^T)Xv = \\gamma(Xv) $$\n",
    "$$ \\Sigma(Xv) = \\gamma(Xv) $$\n",
    "\n",
    "### PCA ALGORITHM 3: SINGULAR VALUE DECOMPOSITION OF THE DATA MATRIX\n",
    "\n",
    "Singular Value Decomposition of the centered data matrix $X$.\n",
    "\n",
    "$X = [x_1,\\dots,x_m] \\in \\mathbb{R}^{Nxm}$ where $N$ is dimension of data and $m$ the number of instances.\n",
    "$$ X_{features * samples} = U \\cdot S \\cdot V^T  $$\n",
    "\n",
    "* Columns of $U$: the principal vectors, ${u^{(1)},\\dots,u^{(k)}}$. Orthogonal and has unit norm, so $U^TU=I$. We can reconstruct the data using linear combination of ${u^{(1)},\\dots,u^{(k)}}$\n",
    "* Matrix $S$: this is a diagonal matrix that shows the importance of each eigenvector\n",
    "* Columns of $V^T$: these are the coefficients for reconstructing the samples\n",
    "\n",
    "## THE PERCEPTRON\n",
    "\n",
    "The perceptron is the father of deep learning.\n",
    "$$ f(x) = \\sigma(\\langle w, x \\rangle + b) $$\n",
    "We have a weightened linear combination ($w$), a non-linear decision function ($\\sigma$) and a linear offset bias ($b$). So, in the end, we have a linear separating hyperplane, which means a binary classification. The learning rule is to estimate the parameters $w$ and $b$.\n",
    "\n",
    "```\n",
    "\n",
    "initialize w=0, b=0\n",
    "repeat\n",
    "    if y_i[<w, x_i> + b] <= 0 then\n",
    "        w = w + y_i*x_i\n",
    "        b = b + y_i\n",
    "    end if\n",
    "until all classified correctly\n",
    "\n",
    "```\n",
    "\n",
    "Nothing happens if the data are already correctly classified. Weight vector is linear combination $w=\\sum_{i \\in I} y_ix_i$. Classifier is linear combination of inner products $ f(x) = \\sum_{i \\in I}y_i\\langle x, x_i \\rangle + b$\n",
    "\n",
    "### CONVERGENCE THEOREM\n",
    "\n",
    "If there exists some $(w^*, b^*)$ with unit length and $y_i[\\langle w*, x_i \\rangle + b^*] \\geq \\rho $ for all $i$, then the perceptron converges to a linear separator after a number of steps bounded by: \n",
    "$$ (1+{b^*}^2)(1+R^2)\\rho^{-2} \\longrightarrow where \\|x_i\\| \\leq R$$\n",
    "\n",
    "$\\rho$ is the distance between closest point to hyperplane and hyperplane, which means $\\underset{i}{min}(y_ix_iw)$.\n",
    "\n",
    "$b$ is the distance of the hyperplane from the origin.\n",
    "\n",
    "$R=\\underset{i}{max}(\\|x_i\\|)$ radius of the hypersphere containing all trainin data.\n",
    "\n",
    "We define alignment $\\langle (w_i, b_i),(w^*,b^*) \\rangle$\n",
    "\n",
    "* STEP 0: $ w_1 = 0 $ and $ b_1 = 0 $\n",
    "* STEP 1: for error in observation in $(x_i, y_i)$ we get a lower bound:\n",
    "\n",
    "$$ \\langle (w_{j+1}, b_{j+1}),(w^*,b^*) \\rangle = \\langle [(w_j, b_j) + y_i(x_i, 1)],(w^*,b^*) \\rangle = $$\n",
    "$$ = \\langle (w_j, b_j),(w^*,b^*) \\rangle + y_i \\langle (x_i, 1),(w^*,b^*) \\rangle $$\n",
    "$$ \\geq \\langle (w_j, b_j),(w^*,b^*) \\rangle + \\rho \\geq j \\cdot \\rho $$\n",
    "\n",
    "* STEP 2: we use the Cauchy-Schwartz inequality for dot product\n",
    "\n",
    "$$ \\langle (w_{j+1}, b_{j+1}),(w^*,b^*) \\rangle \\leq \\|(w_{j+1}, b_{j+1})\\| \\cdot \\|(w^*, b^*)\\| \\leq \\|(w_{j+1}, b_{j+1})\\| \\cdot \\sqrt{1+{b^*}^2} $$\n",
    "\n",
    "* STEP 3: upper bound on $\\|(w_j, b_j)\\|$. If we make a mistake, we have:\n",
    "\n",
    "$$ \\|(w_{j+1}, b_{j+1})\\|^2 = \\|(w_j, b_j) + y_i(x_i, 1)\\|^2 = \\|(w_j, b_j)\\|^2 +2y_i\\langle (x_i, 1),(w_j, b_j) \\rangle + \\|(x_i, 1)\\|^2 $$\n",
    "$$ \\leq \\|(w_j, b_j)\\|^2 + \\|x_i, 1\\|^2 \\leq j(1+R^2) $$\n",
    "\n",
    "* STEP 4: combination of the first three steps\n",
    "\n",
    "$$ j\\rho \\leq \\sqrt{1+{b^*}^2} \\cdot \\|(w_{j+1}, b_{j+1})\\| $$\n",
    "$$ j\\rho \\leq \\sqrt{(1+{b^*}^2)(1+R^2)j} $$\n",
    "$$ j^2\\rho^2 \\leq (1+{b^*}^2)(1+R^2)j \\longrightarrow j \\leq (1+{b^*}^2)(1+R^2)\\rho^{-2} $$\n",
    "\n",
    "## CONSEQUENCES AND NONLINEARITY\n",
    "\n",
    "This method does not depend from dimensionality and order, and it scales with \"difficulty\" of problem. We only need to store errors (this gives a compression bound for perceptron). If margin is too small, the problem is harder. If we have very scattered data, the problem is harder.\n",
    "\n",
    "We can map data (if they are not separable) into feature space $x \\to \\phi(x)$ and solve problem in this space. Query replace $\\langle x, x' \\rangle$ by $\\langle \\phi(x), \\phi(x') \\rangle$ for code.\n",
    "\n",
    "ALGORITMO\n",
    "\n",
    "Nothing happens if classified correctly. Weight vector is linean combination $w=\\sum_{i \\in I}y_i\\phi(x_i)$. Classifier is linear combination of inner products $f(x) = \\sum_{i \\in I} y_i\\langle \\phi(x_i), \\phi(x) \\rangle + b$\n",
    "\n",
    "## KERNELS\n",
    "\n",
    "$$ \\phi(x) := (x_1^2, \\sqrt{2}x_1x_2, x_2^2) $$\n",
    "$$ \\langle \\phi(x), \\phi(x') \\rangle = \\langle (x_1^2,\\sqrt{2}x_1x_2,x_2^2), ({x'}_1^2,\\sqrt{2}x'_1x'_2,{x'}_2^2) \\rangle = \\langle x, x' \\rangle^2 $$\n",
    "\n",
    "Extracting features can sometimes be very costly. For example: second order features in 1000 dimensions! This leads to $5\\cdot10^5$ numbers. For higher order polynomial features much worse. The solution is to don't compute the features but try to compute dot products implicitly. A kernel function $k: X x X \\to \\mathbb{R}$ is a symmetric function in its arguments for which the following property holds $k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle$ for some feature map $\\phi$.\n",
    "\n",
    "### THE KERNEL PERCEPTRON \n",
    "\n",
    "If $k(x, x')$ is much cheaper to compute than $\\phi(x)$\n",
    "\n",
    "ALGORITMO\n",
    "\n",
    "Nothing happens if classified correctly. Weight vector is linean combination $w=\\sum_{i \\in I}y_i\\phi(x_i)$. Classifier is linear combination of inner products $f(x) = \\sum_{i \\in I} y_i\\langle \\phi(x_i), \\phi(x) \\rangle + b = \\sum_{i \\in I} y_ik(x_i, x) + b $\n",
    "\n",
    "### POLYNOMIAL KERNELS\n",
    "\n",
    "We want to extend $k(x, x') = \\langle x, x' \\rangle^2 $ to $ k(x, x') = (\\langle x, x' \\rangle + c)^d $ where $c>0$ and $d \\in \\mathbb{R}$. Prove that such a kernel corresponds to a dot product. Compute the explicit sum given by the kernel:\n",
    "\n",
    "$$ k(x, x') = (\\langle x, x' \\rangle)^d = \\sum_{i=0}^{m} \\begin{pmatrix} d \\\\ i \\end{pmatrix}(\\langle x, x' \\rangle)^ic^{d-i} $$\n",
    "\n",
    "Individual terms $(\\langle x, x' \\rangle)^i$ are dot products for some $\\phi_i(x)$.\n",
    "\n",
    "* Computability: we have to be able to compute $k(x, x')$ efficiently.\n",
    "* Nice and useful functions: the features themselves have to be useful for the learning problem.\n",
    "* Simmetry: $k(x, x') = k(x', x)$\n",
    "\n",
    "Is there always a $\\phi$ such that $k$ really is a dot product?\n",
    "\n",
    "### MERCER'S THEOREM \n",
    "\n",
    "For any symmetric function $k: X x X \\to \\mathbb{R}$ which is square integrable in $XxX$ and which satisfies: \n",
    "\n",
    "$\\int_{XxX} k(x, x')f(x)f(x')dxdx'$ for all $f \\in L_2(X)$\n",
    "\n",
    "There exists $\\phi_i : X \\to \\mathbb{R}$ and numbers $\\lambda_i$ where\n",
    "\n",
    "$ k(x, x') = \\sum_{i}\\lambda_i \\phi_i(x) \\phi_i(x')$ for all $x, x' \\in X$\n",
    "\n",
    "INTERPRETATION: double integral is the continuous version of a vector-matrix-vector multiplication. For positive semidefinite matrices we have:\n",
    "$ \\sum_{i}\\sum_{j}k(x_i, x_j)\\alpha_i\\alpha_j \\geq 0 $\n",
    "\n",
    "#### PROPERTIES\n",
    "\n",
    "Distance between points in feature space via:\n",
    "\n",
    "$$ d(x, x')^2 = \\|\\phi(x)-\\phi(x')\\|^2 = \\langle \\phi(x), \\phi(x) \\rangle - 2\\langle \\phi(x), \\phi(x') \\rangle + \\langle \\phi(x), \\phi(x') \\rangle = k(x, x) + k(x', x') -2k(x, x') $$\n",
    "\n",
    "To compare observations we compute dot products, so we study the matrix $K$ given by $K_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle = k(x_i, x_j)$ where $x_i$ are the training patterns.\n",
    "\n",
    "The entries $K_{ij}$ tell us the overlap between $\\phi(x_i)$ and $\\phi(x_j)$, so $k(x_i, x_j)$ is a similarity measure.\n",
    "\n",
    "$K$ is positive semidefinite $\\to$ claim: $\\alpha^TK\\alpha \\geq 0$ for all $\\alpha \\in \\mathbb{R}^m$ and all kernel matrices $K \\in \\mathbb{R}^{mxm}$\n",
    "\n",
    "$$ \\sum_{i,j}^{m}\\alpha_i\\alpha_jK_{ij} = \\sum_{i,j}^{m}\\alpha_i\\alpha_j \\langle \\phi(x_i), \\phi(x_j) \\rangle = $$\n",
    "\n",
    "$$ = \\langle \\sum_{i}^{m}\\alpha_i\\phi(x_i), \\sum_{j}^{m}\\alpha_j\\phi(x_j) \\rangle = \\| \\sum_{i=1}^{m}\\alpha_i\\phi(x_i) \\|^2 $$\n",
    "\n",
    "If $w$ is given by a linear combination of $\\phi(x_i)$ we get:\n",
    "$$ \\langle w, \\phi(x) \\rangle = \\langle \\sum_{i=1}^{m}\\alpha_i\\phi(x_i), \\phi(x) \\rangle = \\sum_{i=1}^{m}\\alpha_ik(x_i, x) $$\n",
    "\n",
    "If the matrix $K$ have one negative eigenvalue, then the kernel $k$ is not a proper kernel. To check Mercer's condition compute the Fourier transformation of the kernel and check that is nonnegative.\n",
    "\n",
    "Example of kernels:\n",
    "* Linear $\\to \\langle x, x' \\rangle $\n",
    "* Laplacian RBF $\\to e^{-\\lambda\\|x-x'\\|}$\n",
    "* Gaussian RBF $\\to e^{-\\lambda\\|x-x'\\|^2}$\n",
    "* Polynomial $\\to (\\langle x, x' \\rangle + c)^d $\n",
    "* B_Spline $\\to B_{2n+1}(x-x') $\n",
    "* Cond. Expectation $\\to E_c[P(x|c)P(x'|c)] $\n",
    "\n",
    "## DENSITY ESTIMATION\n",
    "\n",
    "For Density Estimation, we observe some data $x_i$. We want to estimate $P(x)$:\n",
    "* Find unusual observations (e.g. security)\n",
    "* Find typical observations (e.g. prototypes)\n",
    "* Classifier via Bayes Rule $\\to P(y|x) = \\frac{P(x, y)}{\\sum_{y'}P(x|y')P(y')}$\n",
    "* Need tool for computing $P(x)$ easily\n",
    "\n",
    "## BIN COUNTING \n",
    "\n",
    "If we have discrete random variables, we can use Bin Counting (record # of occurrences), but there is the problem we can have not enough data. For discrete random variables, #bins grows exponentially but for continuous random variables we need many bins per dimension (almost infinite).\n",
    "\n",
    "$10$ bins on $[0,1]$ is probably good\n",
    "\n",
    "$10^{10}$ bins on $[0,1]^{10}$ requires high accuracy in estimation \n",
    "\n",
    "## PARZEN WINDOWS\n",
    "\n",
    "In Parzen Windows there is a Naive approach. We use empirical density(delta distributions):\n",
    "\n",
    "$$ P_{emp}(x) = \\frac{1}{m}\\sum_{i=1}^{m}\\delta_{x_i}(x) $$\n",
    "\n",
    "This breaks if we see slightly different instances.\n",
    "\n",
    "Kernel density estimate $\\to$ smear out empirical density with a nonnegative smoothing kernel $k_x(x')$ satisfying $\\int_{x}k_x(x')dx' = 1$ for all $x$.\n",
    "\n",
    "Example of smoothing kernels: Gauss, Laplace, Epanechikov, Uniform.\n",
    "\n",
    "## NEAREST NEIGHBOR\n",
    "\n",
    "Table lookup $\\to$ for previously seen instance remember label.\n",
    "\n",
    "Nearest neighbor $\\to$ pick label of most similar neighbor.\n",
    "\n",
    "We have a slight improvement using k-nearest neighbors but this method is good only for small amount of data (because of computations of all possible dinstances). If we get more data:\n",
    "* 1 Nearest Neighbor $\\to$ converges to perfect solution if data detached. twice the minimal error rate $2P(1-P)$ for noisy problems. \n",
    "* k Nearest Neighbors $\\to$ converges to perfect solution if data detached (but it needs more data). Converges to minimal error $min(P, 1-P)$ for noisy problems.\n",
    "\n",
    "## SUPPORT VECTOR MACHINE\n",
    "### Large Margin Classifier\n",
    "\n",
    "$$ f(x)\\to \\langle w,x \\rangle + b = 0 $$\n",
    "$$ x_+\\to \\langle w,x \\rangle + b = 1 $$\n",
    "$$ x_-\\to \\langle w,x \\rangle + b = -1 $$\n",
    "$$ \\frac{\\langle x_+ - x_- , w \\rangle}{2\\|w\\|} = \\frac{1}{2\\|w\\|}[\\langle x_+,w \\rangle - \\langle x_-,w \\rangle +b -b] = \\frac{1}{\\|w\\|} $$\n",
    "\n",
    "\n",
    "$\\underset{w,b}{maximize}\\frac{1}{\\|w\\|}$ subject to $y_i[\\langle x_i,w \\rangle + b] \\geq 1$\n",
    "\n",
    "$\\underset{w,b}{minimize}\\frac{1}{2}\\|w\\|^2$ subject to $y_i[\\langle x_i,w \\rangle + b] \\geq 1$\n",
    "\n",
    "Lagrange Multipliers $\\to$ we want to maximize $f(x,y)$ subject to $g(x,y)=0$. We can use Lagrange function $\\mathcal{L}(w,b,\\alpha) = f(x,y) - \\lambda g(x,y)$\n",
    "\n",
    "$\\mathcal{L}(w,b,\\alpha) = \\frac{1}{2}\\|w\\|^2 - \\sum_{i}\\alpha_i[y_i[\\langle x_i,w \\rangle + b] -1]$\n",
    "\n",
    "Optimality in $w,b$ is at saddle point with $\\alpha$. Derivatives in $w,b$ need to vanish\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial w} = \\|w\\| - \\sum_{i}\\alpha_iy_ix_i = 0 \\to \\|w\\| = \\sum_{i}\\alpha_iy_ix_i $$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum_{i}\\alpha_iy_i = 0 $$\n",
    "\n",
    "Plugging terms back into L yields:\n",
    "\n",
    "$$ \\frac{1}{2}(\\sum_{i}\\alpha_iy_ix_i)^2 - \\sum_{i}\\alpha_i[y_i[\\langle x_i, \\sum_{j}\\alpha_jy_jx_j \\rangle + b] - 1] = $$\n",
    "\n",
    "$$ \\frac{1}{2}\\sum_{ij}\\alpha_i\\alpha_jy_iy_jx_ix_j - \\sum_{ij}\\alpha_i\\alpha_jy_iy_jx_ix_j - \\sum_{i}\\alpha_iy_ib + \\sum_{i}\\alpha_i = $$\n",
    "\n",
    "$$ = -\\frac{1}{2}\\sum_{ij}\\alpha_i\\alpha_jy_iy_jx_ix_j + \\sum_{i}\\alpha_i $$\n",
    "\n",
    "The problem become:\n",
    "\n",
    "$\\underset{\\alpha}{maximize}-\\frac{1}{2}\\sum_{ij}\\alpha_i\\alpha_jy_iy_jx_ix_j + \\sum_{i}\\alpha_i$ subject to $\\sum_{i}\\alpha_iy_i=0$ and $\\alpha_i\\geq 0$\n",
    "\n",
    "$w$ is the vector that defines SVM and can be written with the linear combination of the data, the labels and the coefficient $\\alpha \\longrightarrow w = \\sum_{i}\\alpha_iy_ix_i$. Most of the $\\alpha$ are zeros, except the points of the two classes that lie on the margins. In the end, we are minimizing the risk of making mistakes on my training data.\n",
    "\n",
    "### Karush Kuhn Tucker optimality condition\n",
    "\n",
    "$$ \\alpha_i[y_i[\\langle w, x_i \\rangle + b] - 1] = 0 $$\n",
    "\n",
    "Weight vector $w$ as weightened linear combination of instances. Only points on margin matter(ignore the rest and get same solution). Only inner products matter(quadratic program, we can replace the inner products by a kernel). Keeps instances away from the margin.\n",
    "\n",
    "Why large margins? It gives you the maximum robustness relative to uncertainty(best performance I can get with training data). I have symmetry breaking. Large margins are independent of correctly classified instances and they are easy to find for easy problems.\n",
    "\n",
    "## Soft Margins with slack variables\n",
    "\n",
    "$$ \\langle w,x \\rangle + b \\leq -1+\\xi $$\n",
    "\n",
    "$$ \\langle w,x \\rangle + b \\geq 1-\\xi $$\n",
    "\n",
    "We still have a convex optimization problem. I have to maximize the margins while minimizing the amount of slack variables $\\xi$. The problem is always feasible.\n",
    "\n",
    "$\\underset{w,b}{minimize}\\frac{1}{2}\\|w\\|^2 + C\\sum_{i}\\xi_i$ subject to $y_i[\\langle w, x_i \\rangle +b] \\geq 1 - \\xi_i$ and $\\xi \\geq 0$\n",
    "\n",
    "$$ \\mathcal{L}(w,b,\\alpha,\\eta) = \\frac{1}{2}\\|w\\|^2 + C\\sum_{i}\\xi_i -\\sum_{i}\\alpha_i[y_i[\\langle x_i,w \\rangle + b] -1 +\\xi_i] -\\sum_{i}\\eta_i\\xi_i $$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial w} = \\|w\\| - \\sum_{i}\\alpha_iy_ix_i = 0 \\to \\|w\\| = \\sum_{i}\\alpha_iy_ix_i $$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum_{i}\\alpha_iy_i = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = C-\\alpha_i-\\eta_i = 0 $$\n",
    "\n",
    "The problem become:\n",
    "\n",
    "$\\underset{\\alpha}{maximize}-\\frac{1}{2}\\sum_{ij}\\alpha_i\\alpha_jy_iy_jx_ix_j + \\sum_{i}\\alpha_i$ subject to $\\sum_{i}\\alpha_iy_i=0$ and $\\alpha \\in [0,C]$\n",
    "\n",
    "### Karush Kuhn Tucker conditions \n",
    "\n",
    "$$ \\alpha_i[y_i[\\langle x_i,w \\rangle + b] -1 +\\xi] = 0 $$\n",
    "\n",
    "$$ \\eta_i\\xi_i = 0 $$\n",
    "\n",
    "* $\\alpha_i = 0 \\longrightarrow y_i[\\langle x_i,w \\rangle + b] \\geq 1$\n",
    "* $0 <\\alpha_i < C \\longrightarrow y_i[\\langle x_i,w \\rangle + b] = 1$\n",
    "* $\\alpha_i = C \\longrightarrow y_i[\\langle x_i,w \\rangle + b] \\leq 1$\n",
    "\n",
    "### The Kernel trick\n",
    "\n",
    "$\\underset{w,b}{minimize}\\frac{1}{2}\\|w\\|^2 + C\\sum_{i}\\xi_i$ subject to $y_i[\\langle w, \\phi(x_i) \\rangle +b] \\geq 1 - \\xi_i$ and $\\xi \\geq 0$\n",
    "\n",
    "$\\underset{\\alpha}{maximize}-\\frac{1}{2}\\sum_{ij}\\alpha_i\\alpha_jy_iy_jk(x_i, x_j) + \\sum_{i}\\alpha_i$ subject to $\\sum_{i}\\alpha_iy_i=0$ and $\\alpha \\in [0,C]$\n",
    "\n",
    "Support Vector Expansion $\\longrightarrow f(x) = \\sum\\alpha_iy_ik(x_i, x) +b$\n",
    "\n",
    "## REGRESSION\n",
    "\n",
    "In everyday life we need to make decisions by taking into account lots of factor. The question is what weight we put on each of these factors(how important are they with respect to others). If we have many observations we may be able to recover the weights.\n",
    "\n",
    "### LINEAR REGRESSION\n",
    "\n",
    "We try to predict not the category but the real value. Given an input $x$ we would like to compute an output $y$. In linear regression we assume that $y$ and $x$ are related with the following equation:\n",
    "$$ y = wx +\\epsilon $$\n",
    "\n",
    "Where:\n",
    "* $y\\to$ is what we want to predict \n",
    "* $w\\to$ is the parameter\n",
    "* $x\\to$ are the observed values\n",
    "* $\\epsilon\\to$ is noise/some other measurements\n",
    "\n",
    "Our goal is to estimate $w$ from a training data of $(x_i, y_i)$ pairs. One way to find such relationship is to minimize the least square error, but several other approaches can be used as well. So, why use least square?\n",
    "* minimizes squared distance between measurements and predicted line\n",
    "* has a nice probabilistic interpretation\n",
    "* easy to compute\n",
    "\n",
    "If the noise is Gaussian with mean 0 then least squares is also the MLE of $w$. $\\underset{w}{argmin}\\sum_{i}(y_i, wx_i)^2$\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial w}\\sum_{i}(y_i - wx_i)^2 = 2\\sum_{i}-x_i(y_i-wx_i) \\to 2\\sum_{i}x_i(y_i-wx_i) = 0 \\to \\sum_{i}x_iy_i = \\sum_{i}wx_i^2 \\to w = \\frac{\\sum_{i}x_iy_i}{\\sum_{i}x_i^2} $$\n",
    "\n",
    "So far we assumed that the line passes through the origin. What if the line does not? No problem, simply change the model to: $y = w_0 + w_1x +\\epsilon$. We can use least squares to determine $w_0,w_1$:\n",
    "\n",
    "$$ w_0 = \\frac{\\sum_{i}y_i - w_1x_i}{n} $$\n",
    "$$ w_1 = \\frac{\\sum_{i}x_i(y_i-w_0)}{\\sum_{i}x_i^2} $$\n",
    "\n",
    "### MULTIVARIATE REGRESSION\n",
    "\n",
    "What if we have several inputs? This becomes a multivariate linear regression problem. Again, it's easy to model: $y=w_0 + w_1x_1 + \\dots + w_kx_k + \\epsilon$. Not all functions can be approximated using the input values directly. In some cases, we would like to use polynomial or other terms based on the input data, are these still linear regression problem? Yes! As long as the coefficients are linear, the equation is still a linear regression problem!.\n",
    "\n",
    "So far, we only used the observed values. However, linear regression can be applied in the same way to function of these values. As long as these functions can be directly computed from the observed values, the parameters are still linear in the data and the problem remains a linear regression problem.\n",
    "* Polynomial: $\\phi_j(x) = x^j$ for $j=0,1,\\dots,n$\n",
    "* Gaussian: $\\phi_j(x) = \\frac{(x-\\mu_j)}{2\\sigma^2_j}$\n",
    "* Sigmoid: $\\phi_j(x) = \\frac{1}{1+e^{-s_jx}}$\n",
    "\n",
    "### GENERAL LINEAR REGRESSION PROBLEM\n",
    "\n",
    "Using our new notations for tha basis linear regression, it can be written as:\n",
    "$$y=\\sum_{j=0}^{n}w_j\\phi_j(x)$$\n",
    "\n",
    "Where $\\phi_j(x)$ can be either $x_j$ for multivariate regression or one of the non linear basis we defined. Once again we use least squares method. Our goal is to minimize the loss function.\n",
    "\n",
    "$$ J(w) = \\sum_{i}(y_i - \\sum_{j}w_j\\phi_j(x_i))^2 $$\n",
    "$$ y = \\sum_{j=0}^{n}w_j\\phi_j(x) $$\n",
    "\n",
    "Moving to vector notation we have:\n",
    "\n",
    "$$ J(w) = \\sum_{i}(y_i - w^T\\phi(x_i))^2 $$\n",
    "$$ \\frac{\\partial J}{\\partial w} = 2\\sum_{i}(y_i - w^T\\phi(x_i))\\phi(x_i)^T \\to 2\\sum_{i}(y_i - w^T\\phi(x_i))\\phi(x_i)^T = 0 $$\n",
    "$$ \\sum_{i}y_i\\phi(x_i)^T = w^T\\sum_{i}\\phi(x_i)\\phi(x_i)^T $$\n",
    "\n",
    "We define: \n",
    "\n",
    "$$\\Phi = \\begin{bmatrix} \\phi_0(x_1) & \\cdots & \\phi_m(x_1) \\\\ \\vdots & \\ddots & \\vdots \\\\ \\phi_0(x_n) & \\cdots & \\phi_m(x_n) \\end{bmatrix}$$\n",
    "\n",
    "Then we get:\n",
    "\n",
    "$$ w = \\frac{\\Phi^Ty}{\\Phi^T\\Phi} $$\n",
    "\n",
    "This solution is also known as \"pseudo inverse\". Our least squares minimization solution can also be motivated by a probabilistic interpretation: the MLE for $w$ in this model is the same as the solution we derived for least squares criteria. Linear regression is a useful model for many problems. However, the parameters we learn are GLOBAL; they are the same regardless of the value of input $x$.\n",
    "\n",
    "#### Generative vs Discriminative Classifiers\n",
    "\n",
    "When using generative classifiers we relied on all points to learn the generative model.\n",
    "\n",
    "When using discriminative classifiers we mainly care about the boundaries.\n",
    "\n",
    "### REGRESSION FOR CLASSIFICATION\n",
    "\n",
    "In some cases we can use linear regression for determining the appropriate boundary. However, since the output is usually binary or discrete there are more efficient regression methods. Recall that for classification we are interested in the conditional probability $P(y|x;\\theta)$ where $\\theta$ are the parameters of our model. When using regression, $\\theta$ represents the values of our regression coefficients ($w$). To classify using regression models we replace the linear function with the Sigmoid, $g(h) = \\frac{1}{1+e^{-h}}$ (for binary classification):\n",
    "\n",
    "$$ P(y=0|x;\\theta) = g(w^Tx) = \\frac{1}{1+e^{w^Tx}} $$\n",
    "$$ P(y=1|x;\\theta) = 1-g(w^Tx) = \\frac{e^{w^Tx}}{1+e^{w^Tx}} $$\n",
    "\n",
    "So, how do we learn parameters? Similar to other regression problems, we look for the MLE for $w$. The Likelihood(L) of the data given the model is:\n",
    "\n",
    "$$ L(y|x;w) = \\prod_{i}(1-g(x_i;w))^{y_i}\\cdot g(x_i;w)^{1-y_i} $$\n",
    "\n",
    "Taking the log we get:\n",
    "\n",
    "$$ LL(y|x;w) = \\sum_{i=1}^{N}y_iln(1-g(x_i;w))+(1-y_i)ln(g(x_i;w)) = \\sum_{i=1}^{N}y_iln(1-g(x_i;w))-y_iln(g(x_i;w))+ln(g(x_i;w)) = $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{N}y_iln\\left(\\frac{1-g(x_i;w)}{g(x_i;w)}\\right) + ln(g(x_i;w)) = \\sum_{i=1}^{N}y_iw^Tx_i - ln(1+e^{w^Tx_i}) = \\ell(w)$$\n",
    "\n",
    "$$ \\frac{\\partial \\ell(w)}{\\partial w_j} = \\sum_{i=1}^{N}x_i^j[y_i - (1 - g(x_i;w))] = \\sum_{i=1}^{N}x_i^j[y_i - P(y_i=1|x_i;w)] $$\n",
    "\n",
    "The bad news are that there are not close form solution! But the good news is that we have a concave function.\n",
    "\n",
    "### GRADIENT ASCENT\n",
    "\n",
    "\n",
    "We use the gradient ascent to adjuste the value of $w$ (note that $\\epsilon$ is a small constant):\n",
    "$$ w_j \\longleftarrow w_j + \\epsilon\\sum_{i=1}^{N}x_i^j[y_i - (1-g(x_i;w))] $$\n",
    "\n",
    "### GRADIENT DESCEND\n",
    "\n",
    "### Algorithm for Logistic Regression\n",
    "1. Choose $\\lambda$\n",
    "2. Start with a guess for $w$\n",
    "3. For all $j$ set $w_j \\longleftarrow w_j + \\epsilon\\sum_{i=1}^{N}x_i^j[y_i - (1-g(x_i;w))]$\n",
    "4. If no improvement for $LL(y|x;w)$ stop. Otherwise, go to step 3\n",
    "\n",
    "### REGULARIZATION\n",
    "\n",
    "Similar to other data estimation problems, we may not have enough samples to learn good models for logistic regression classification. One way to overcome this is to regularize the model, impose additional constraints on the parameters we are fitting. For example, lets assume that $w_i$ comes from a Gaussian distribution with mean 0 and variace $\\sigma^2$. In that case we have a prior on the parameters and so: \n",
    "\n",
    "$$P(y=1, \\theta | x) \\propto P(y=1|x;\\theta)P(\\theta)$$\n",
    "\n",
    "If we regularize, we need to take the prior into account when computing the posterior for our parameters. Here we use Gaussian model for the prior. Thus, the log likelihood changes to:\n",
    "\n",
    "$$LL(y;w|x) = \\sum_{i=1}^{N}y_iw^Tx_i - ln(1+e^{w^Tx_i}) - \\sum{j}\\frac{(w_j)^2}{2\\sigma^2} $$\n",
    "\n",
    "And then the new update rule is:\n",
    "\n",
    "$$ w_j \\longleftarrow w_j + \\epsilon\\sum_{i=1}^{N}x_i^j[y_i-(1-g(x_i;w))] - \\epsilon\\frac{w_j}{\\sigma^2} $$\n",
    "\n",
    "## PERCEPTRON LEARNING ALGORITHM\n",
    "\n",
    "$$ net = \\sum_{i=0}^{n}w_ix_i $$\n",
    "\n",
    "$$ o = \\sigma(net) = \\frac{1}{1+e^{-net}} \\longrightarrow \\hat{f} $$\n",
    "\n",
    "Recall the nice property of Sigmoid function: $\\frac{d\\sigma}{dnet} = \\sigma(1-\\sigma)$.\n",
    "\n",
    "Consider regression problem $f:X\\to Y$, for scalar $y: y=f(x)+\\epsilon$. We used to maximize the conditional data likelihood.\n",
    "\n",
    "$$ \\vec{w} = \\underset{\\vec{w}}{argmax}ln\\left(\\prod_{i}P(y_i|x_i;\\vec{w})\\right) $$\n",
    "\n",
    "$$ \\vec{w} = \\underset{\\vec{w}}{argmin}\\sum_{i}\\frac{1}{2}(y_i - \\hat{f}(x_i;\\vec{w})) \\longrightarrow E(\\vec{w}) = \\sum_{i}(y_i - \\hat{f}(x_i;\\vec{w})) $$\n",
    "\n",
    "We consider $x_d =$ input and $t_d =$ target output and $o_d =$ observed output.\n",
    "\n",
    "$$ \\frac{\\partial E[\\vec{w}]}{\\partial w_j} = \\frac{\\partial}{\\partial w_j}\\frac{1}{2}\\sum(t_d-o_d)^2 $$\n",
    "\n",
    "$$ \\nabla E[\\vec{w}] = \\begin{bmatrix} \\frac{\\partial E}{\\partial w_0} & \\frac{\\partial E}{\\partial w_1} & \\cdots & \\frac{\\partial E}{\\partial w_n} \\end{bmatrix} $$\n",
    "\n",
    "The training rule is $\\Delta\\vec{w} = -\\eta\\nabla E[\\vec{w}]$\n",
    "\n",
    "$$ \\frac{\\partial E_d[\\vec{w}]}{\\partial w_j} = \\frac{1}{2}\\sum_{d}2(t_d-o_d)\\left( - \\frac{\\partial o_d}{\\partial w_j}\\right) = -\\sum_{d}(t_d-o_d) \\frac{\\partial o_d}{\\partial w_j} \\frac{\\partial net}{\\partial net} = -\\sum_{d}(t_d-o_d)o_d(1-o_d)x_d^i$$\n",
    "\n",
    "### Batch Mode\n",
    "\n",
    "Do until converge:\n",
    "1. Compute gradient $\\nabla E_d[\\vec{w}]$\n",
    "2. $\\vec{w} = \\vec{w} - \\eta\\nabla E_d[\\vec{w}]$\n",
    "\n",
    "### Incremental Mode\n",
    "\n",
    "Do until converge: \n",
    "1. For EACH training example $d \\in D$\n",
    " 1. Compute gradient $\\nabla E_d[\\vec{w}]$\n",
    " 2. $\\vec{w} = \\vec{w} - \\eta\\nabla E_d[\\vec{w}]$\n",
    " \n",
    "where $\\nabla E_d[\\vec{w}] = -(t_d-o_d)o_d(1-o_d)\\vec{x_d}$\n",
    "\n",
    "### MLE VS MAP\n",
    "\n",
    "* Maximum Likelihood Estimation $$\\vec{w} = \\underset{\\vec{w}}{argmax}[ln(\\prod_{i}P(y_i|x_i;\\vec{w}))]$$ $$\\vec{w} \\longleftarrow \\vec{w} +\\eta \\sum_{d}(t_d-o_d)o_d(1-o_d)\\vec{x_d}$$\n",
    "* Maximum A Posteriori Estimation $$\\vec{w} = \\underset{\\vec{w}}{argmax}[ln(P(\\vec{w})\\prod_{i}P(y_i|x_i;\\vec{w}))]$$ $$\\vec{w} \\longleftarrow \\vec{w} +\\eta (\\sum_{d}(t_d-o_d)o_d(1-o_d)\\vec{x_d} -\\lambda\\vec{w})$$\n",
    "\n",
    "### BACKPROPAGATION ALGORITHM\n",
    "\n",
    "Initialize all weights to small random numbers. Until convergence do:\n",
    "\n",
    "1. Input the training example to the network and compute the network outputs\n",
    "2. For each output unit k $$\\delta_k\\leftarrow o_k^2(1-o_k^2)(t-o_k^2)$$\n",
    "3. For each hidden unit h $$\\delta_h\\leftarrow o_h^1(1-o_h^1)\\sum_{k} w_{h,k}\\delta_k$$\n",
    "4. Update each network weight $w_{i,j}$ $$w_{i,j}\\leftarrow w_{i,j}+\\Delta w_{i,j}$$ where $\\Delta w_{i,j}=\\eta\\delta_jx_j$\n",
    "\n",
    "Backpropagation is doing gradient descend over entire network weight vector. Easily generalized to arbitrary directed graphs. It will find a local, not necessarily global error minimum (in pratice, often works well). It often include a weight momentum $\\alpha$:\n",
    "\n",
    "$$ \\Delta w_{i,j}(t) = \\eta\\delta_jx_{i,j} + \\alpha\\Delta w_{i,j}(t-1)$$\n",
    "\n",
    "It minimizes error over training examples but training can take thousands of iterations (very slow!). However, after that training, using the network is really fast. Minimizing sum of squared training errors gives MLE estimates of network weights if we assume zero mean Gaussian noise on output values. Minimizing sum of squared errors plus weight squared (regularization) gives MAP estimates assuming weight priors are zero mean Gaussian.\n",
    "\n",
    "Local minima is the greatest problem!\n",
    "\n",
    "## RISK\n",
    "\n",
    "$D=\\{(x_1, y_1),\\dots,(x_n,y_n)\\} \\longrightarrow$ training data\n",
    "\n",
    "$\\{(x_{n+1}, y_{n+1}), \\dots, (x_m, y_m)\\}\\longrightarrow$ test data\n",
    "\n",
    "Generative model of the data $\\longrightarrow x\\sim\\mu$, $\\mu(A) = P(x \\in A)$, $y \\sim P(\\cdot | x)$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "$L(x, y, f(x))$. $x$ is the input data, $y$ is the true label and $f(x)$ is the predicted label. The loss function function measures how good we are on a particular $(x,y)$ pair. We want the loss $L(x_t, y_t, f(x_t))$ to be small for many $(x_t,y_t)$ pairs in the test data.\n",
    "\n",
    "* Classification Loss $\\longrightarrow L(x, y, f(x)) = \\begin{cases} 1 & \\mbox{if } y = f(x) \\\\ 0 & \\mbox{if } y \\neq f(x)\\end{cases}$\n",
    "* Squared Loss $\\longrightarrow L(y, f(x)) = \\frac{1}{2}(y-f(x))^2$\n",
    "* L1 Loss $\\longrightarrow L(y, f(x)) = |y-f(x)|$\n",
    "* Hinge Loss $\\longrightarrow L(y, f(x)) =max(0, 1-y\\cdot f(x)$\n",
    "\n",
    "### Risk Definition\n",
    "\n",
    "Risk of $f$ classification/regression function:\n",
    "\n",
    "$$R_{L,P} = \\int L(x, y, f(x))dP(x,y) = E[L(x, y, f(x))] = \\mbox{The expected loss}$$\n",
    "\n",
    "Our true goal is to minimize the loss of the test points!\n",
    "\n",
    "$$ f^* = \\underset{f}{argmin}\\frac{1}{m-n}\\sum_{i=n+1}^{m}L(x_i, y_i, f(x_i)) \\overset{m\\to\\infty}{\\longrightarrow} R_{L,P}(f)$$\n",
    "\n",
    "This is verified by the Law of Large Number and that's why our goal is to minimize the risk.\n",
    "\n",
    "### Bayes Risk\n",
    "\n",
    "$$R_{L,P}^* = \\underset{f:x\\to\\mathbb{R}}{inf}\\int L(x, y, f(x))dP(x,y)$$\n",
    "\n",
    "We consider all possible function $f$ here. we don't know $P$, but we have i.i.d. training data sampled from $P$!\n",
    "\n",
    "GOAL OF THE LEARNING $\\to$ build a function $f_d$ (using data D) whose risk $R_{L,P}(f_D)$ will be close to the Bayes Risk $R_{L,P}^*$\n",
    "\n",
    "### Consistency\n",
    "\n",
    "A learning method is universally consistent if for all $P(x,y)$ distributions the risk converges to the Bayes Risk when we increase the sample size.\n",
    "\n",
    "$$ R_{L,P}(f_d) \\overset{P}{\\longrightarrow} R_{L,P}^* \\mbox{  as } n\\to\\infty $$\n",
    "\n",
    "### Stone's theorem\n",
    "\n",
    "Many classifications, regression algorithms are universally consistent for certain loss functions under certain conditions: kNN, Parzen kernel regression, SVM ....\n",
    "\n",
    "This theorem does not tell us anything about the rates..\n",
    "\n",
    "### Devroy's no free lunch theorem\n",
    "\n",
    "For every consistent learning method and for every fixed convergence rate $\\alpha_n,\\exists P(x,y)$ distribution such that the convergence rate of this learning method on $P(x, y)$ distribuited data is slower than $\\alpha_n$\n",
    "\n",
    "$R_{L,P}(f_d) \\overset{P}{\\longrightarrow}R_{L,P}^*$ as $n\\to\\infty$ with slower rate then $\\alpha_n$\n",
    "\n",
    "### Empirical Risk\n",
    "\n",
    "Let $L(x, y, f(x)) = L(y, f(x))$. We don't know $P$, hence we don't know $R(f)$ either. Let us use the empirical counter part.\n",
    "\n",
    "$$ \\hat{R}_n(f) = \\frac{1}{n}\\sum_{i=1}^{n}L(y_i, f(x_i)) $$\n",
    "\n",
    "$$ R(f) = R_{L,P}(f) = E[L(y, f(x))] $$\n",
    "\n",
    "$$ R^* = R_{L,P}^* = \\underset{f:x\\to\\mathbb{R}}{inf}R(f) $$\n",
    "\n",
    "Law of Large Numbers! For each fixed $f$, \n",
    "\n",
    "$$ \\hat{R}_n(f) = \\frac{1}{n}\\sum_{i=1}^{n}L(y_i, f(x_i)) \\overset{n\\to\\infty}{\\longrightarrow}R(f) $$\n",
    "\n",
    "Empirical Risk is coverging to the true risk. We need $\\underset{f:x\\to\\mathbb{R}}{inf}R(f)$, so let us calculate $\\underset{f:x\\to\\mathbb{R}}{inf}\\hat{R}(f)$!\n",
    "\n",
    "$$ \\underset{f:x\\to\\mathbb{R}}{inf}\\hat{R}(f) = \\underset{f:x\\to\\mathbb{R}}{inf}\\frac{1}{n}\\sum_{i=1}^{n}L(y_i,f(x_i)) $$\n",
    "\n",
    "This is a terrible idea!!! Optimize over all possible $f:x\\to\\mathbb{R}$? EXTREME OVERFITTING!!! Solution? Minimize over a smaller function set $F$\n",
    "\n",
    "### Structural Risk\n",
    "\n",
    "$$ f^*_F = \\underset{f\\in F}{argmin}\\frac{1}{n}\\sum_{i=1}^{n}L(y_i, f(x_i))$$\n",
    "\n",
    "$$ R_F^* = \\underset{f\\in F}{inf} E[L(y, f(x))] \\to \\mbox{  Bayes Risk  }$$\n",
    "\n",
    "$$ \\hat{R}^*_{n,F} = \\underset{f\\in F}{inf} \\frac{1}{n}L(y_i, f(x_i)) \\to \\mbox{  Structural Risk  } $$\n",
    "\n",
    "First issue $\\to R_F^* - R^* \\geq 0$ needs to be small (Structural Risk minimization is the solution).\n",
    "\n",
    "Second issue $\\to$ minimizing $\\hat{R}_n(f)$ might be a very difficult optimization problem in $f$. \n",
    "\n",
    "The solution is to choose $L$ such that  $\\hat{R}_n(f)$ will be convex in $f$. \n",
    "\n",
    "As a matter of fact, L2 Loss and Hinge Loss are good Loss functions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
